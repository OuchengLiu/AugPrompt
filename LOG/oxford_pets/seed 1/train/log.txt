***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoCoOp/vit_b16_c16_ep10_batch1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/3/base2new/train_base/oxford_pets/shots_16/CoCoOp/vit_b16_c16_ep10_batch1/seed1
resume: 
root: ../DATA
seed: 1
source_domains: None
target_domains: None
trainer: CoCoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 1
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ../DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_flip',)
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 10
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/3/base2new/train_base/oxford_pets/shots_16/CoCoOp/vit_b16_c16_ep10_batch1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoCoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.2.2+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ¼ÒÍ¥ÖÐÎÄ°æ
GCC version: (x86_64-win32-seh-rev0, Built by MinGW-W64 project) 8.1.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060
Nvidia driver version: 527.54
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=2592
DeviceID=CPU0
Family=198
L2CacheSize=1536
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=2592
Name=Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.26.0
[pip3] torch==2.2.2+cu118
[pip3] torchaudio==2.2.2+cu118
[pip3] torchvision==0.17.2+cu118
[conda] blas                      1.0                         mkl  
[conda] mkl                       2023.1.0         h6b88ed4_46357  
[conda] mkl-service               2.4.0           py310h2bbff1b_1  
[conda] mkl_fft                   1.3.8           py310h2bbff1b_0  
[conda] mkl_random                1.2.4           py310h59b6b97_0  
[conda] numpy                     1.26.0          py310h055cbcc_0  
[conda] numpy-base                1.26.0          py310h65a83cf_0  
[conda] torch                     1.13.1+cu117             pypi_0    pypi
[conda] torchaudio                0.13.1+cu117             pypi_0    pypi
[conda] torchvision               0.14.1                   pypi_0    pypi
        Pillow (9.4.0)

Loading trainer: CoCoOp
Loading dataset: OxfordPets
Reading split from C:\Jupyter\MyConda\Experiment\1_CoOp_\DATA\oxford_pets\split_zhou_OxfordPets.json
Loading preprocessed few-shot data from C:\Jupyter\MyConda\Experiment\1_CoOp_\DATA\oxford_pets\split_fewshot\shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ resize to 224x224
+ random flip
+ to torch tensor of range [0, 1]
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Aid context: "X X X X"
Number of aid context words (tokens): 4
Turning off gradients in both the image and the text encoder
Parameters to be updated: {'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.weight'}
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/3/base2new/train_base/oxford_pets/shots_16/CoCoOp/vit_b16_c16_ep10_batch1/seed1\tensorboard)
epoch [1/10] batch [20/304] time 0.203 (4.990) data 0.000 (4.482) loss 1.9209 (1.3697) lr 1.0000e-05 eta 4:11:11
epoch [1/10] batch [40/304] time 0.203 (2.587) data 0.000 (2.241) loss 0.0003 (1.0331) lr 1.0000e-05 eta 2:09:19
epoch [1/10] batch [60/304] time 0.187 (1.786) data 0.000 (1.494) loss 0.0200 (0.7705) lr 1.0000e-05 eta 1:28:41
epoch [1/10] batch [80/304] time 0.172 (1.386) data 0.000 (1.121) loss 0.0085 (0.6970) lr 1.0000e-05 eta 1:08:22
epoch [1/10] batch [100/304] time 0.219 (1.148) data 0.000 (0.897) loss 0.3059 (0.6286) lr 1.0000e-05 eta 0:56:14
epoch [1/10] batch [120/304] time 0.172 (0.992) data 0.000 (0.747) loss 1.6064 (0.6132) lr 1.0000e-05 eta 0:48:15
epoch [1/10] batch [140/304] time 0.203 (0.879) data 0.000 (0.641) loss 0.0022 (0.5639) lr 1.0000e-05 eta 0:42:28
epoch [1/10] batch [160/304] time 0.187 (0.794) data 0.000 (0.561) loss 0.0732 (0.5294) lr 1.0000e-05 eta 0:38:07
epoch [1/10] batch [180/304] time 0.187 (0.728) data 0.000 (0.498) loss 0.0507 (0.4945) lr 1.0000e-05 eta 0:34:41
epoch [1/10] batch [200/304] time 0.187 (0.675) data 0.000 (0.448) loss 0.9761 (0.5267) lr 1.0000e-05 eta 0:31:57
epoch [1/10] batch [220/304] time 0.172 (0.632) data 0.000 (0.408) loss 0.0245 (0.5009) lr 1.0000e-05 eta 0:29:42
epoch [1/10] batch [240/304] time 0.187 (0.596) data 0.000 (0.374) loss 0.0360 (0.4948) lr 1.0000e-05 eta 0:27:49
epoch [1/10] batch [260/304] time 0.203 (0.567) data 0.000 (0.345) loss 2.0762 (0.5026) lr 1.0000e-05 eta 0:26:15
epoch [1/10] batch [280/304] time 0.250 (0.542) data 0.000 (0.320) loss 0.0100 (0.4947) lr 1.0000e-05 eta 0:24:57
epoch [1/10] batch [300/304] time 0.187 (0.520) data 0.000 (0.299) loss 0.0556 (0.4693) lr 1.0000e-05 eta 0:23:44
epoch [2/10] batch [20/304] time 0.156 (4.611) data 0.000 (4.416) loss 0.6709 (0.6963) lr 2.0000e-03 eta 3:28:42
epoch [2/10] batch [40/304] time 0.203 (2.402) data 0.000 (2.208) loss 0.0209 (0.6365) lr 2.0000e-03 eta 1:47:56
epoch [2/10] batch [60/304] time 0.203 (1.666) data 0.000 (1.472) loss 0.5742 (0.5848) lr 2.0000e-03 eta 1:14:17
epoch [2/10] batch [80/304] time 0.203 (1.298) data 0.000 (1.104) loss 0.4795 (0.6503) lr 2.0000e-03 eta 0:57:27
epoch [2/10] batch [100/304] time 0.203 (1.079) data 0.000 (0.883) loss 0.1267 (0.5772) lr 2.0000e-03 eta 0:47:24
epoch [2/10] batch [120/304] time 0.187 (0.933) data 0.000 (0.736) loss 0.4058 (0.5753) lr 2.0000e-03 eta 0:40:41
epoch [2/10] batch [140/304] time 0.219 (0.829) data 0.000 (0.631) loss 0.0018 (0.5372) lr 2.0000e-03 eta 0:35:51
epoch [2/10] batch [160/304] time 0.203 (0.751) data 0.000 (0.552) loss 0.1533 (0.4889) lr 2.0000e-03 eta 0:32:14
epoch [2/10] batch [180/304] time 0.203 (0.690) data 0.000 (0.491) loss 0.0143 (0.5001) lr 2.0000e-03 eta 0:29:23
epoch [2/10] batch [200/304] time 0.203 (0.641) data 0.000 (0.442) loss 1.9355 (0.4749) lr 2.0000e-03 eta 0:27:06
epoch [2/10] batch [220/304] time 0.219 (0.602) data 0.000 (0.402) loss 0.0067 (0.4567) lr 2.0000e-03 eta 0:25:14
epoch [2/10] batch [240/304] time 0.203 (0.570) data 0.000 (0.368) loss 0.0149 (0.4330) lr 2.0000e-03 eta 0:23:42
epoch [2/10] batch [260/304] time 0.234 (0.542) data 0.000 (0.340) loss 0.1874 (0.4042) lr 2.0000e-03 eta 0:22:23
epoch [2/10] batch [280/304] time 0.225 (0.518) data 0.000 (0.316) loss 0.0311 (0.4008) lr 2.0000e-03 eta 0:21:13
epoch [2/10] batch [300/304] time 0.219 (0.498) data 0.000 (0.295) loss 0.0654 (0.3935) lr 2.0000e-03 eta 0:20:12
epoch [3/10] batch [20/304] time 0.172 (4.733) data 0.000 (4.533) loss 0.0004 (0.0849) lr 1.9511e-03 eta 3:10:17
epoch [3/10] batch [40/304] time 0.203 (2.461) data 0.000 (2.266) loss 0.0171 (0.1141) lr 1.9511e-03 eta 1:38:07
epoch [3/10] batch [60/304] time 0.156 (1.703) data 0.000 (1.511) loss 1.0713 (0.1764) lr 1.9511e-03 eta 1:07:18
epoch [3/10] batch [80/304] time 0.203 (1.325) data 0.000 (1.133) loss 0.0124 (0.2105) lr 1.9511e-03 eta 0:51:57
epoch [3/10] batch [100/304] time 0.172 (1.098) data 0.000 (0.907) loss 0.0038 (0.2005) lr 1.9511e-03 eta 0:42:40
epoch [3/10] batch [120/304] time 0.219 (0.948) data 0.000 (0.755) loss 0.0016 (0.1724) lr 1.9511e-03 eta 0:36:30
epoch [3/10] batch [140/304] time 0.203 (0.842) data 0.000 (0.648) loss 0.1764 (0.1934) lr 1.9511e-03 eta 0:32:09
epoch [3/10] batch [160/304] time 0.203 (0.762) data 0.000 (0.567) loss 0.0111 (0.2312) lr 1.9511e-03 eta 0:28:51
epoch [3/10] batch [180/304] time 0.203 (0.700) data 0.000 (0.504) loss 0.2238 (0.2509) lr 1.9511e-03 eta 0:26:15
epoch [3/10] batch [200/304] time 0.234 (0.652) data 0.000 (0.453) loss 0.0005 (0.2493) lr 1.9511e-03 eta 0:24:15
epoch [3/10] batch [220/304] time 0.219 (0.615) data 0.000 (0.412) loss 0.0000 (0.2461) lr 1.9511e-03 eta 0:22:39
epoch [3/10] batch [240/304] time 0.181 (0.582) data 0.000 (0.378) loss 0.1654 (0.2344) lr 1.9511e-03 eta 0:21:14
epoch [3/10] batch [260/304] time 0.219 (0.552) data 0.000 (0.349) loss 0.0067 (0.2331) lr 1.9511e-03 eta 0:19:59
epoch [3/10] batch [280/304] time 0.203 (0.527) data 0.000 (0.324) loss 0.0191 (0.2304) lr 1.9511e-03 eta 0:18:54
epoch [3/10] batch [300/304] time 0.203 (0.506) data 0.000 (0.302) loss 0.0031 (0.2361) lr 1.9511e-03 eta 0:17:58
epoch [4/10] batch [20/304] time 0.187 (4.467) data 0.000 (4.269) loss 0.0001 (0.0752) lr 1.8090e-03 eta 2:36:55
epoch [4/10] batch [40/304] time 0.187 (2.333) data 0.000 (2.134) loss 0.0064 (0.1702) lr 1.8090e-03 eta 1:21:10
epoch [4/10] batch [60/304] time 0.219 (1.621) data 0.000 (1.423) loss 0.0186 (0.1459) lr 1.8090e-03 eta 0:55:52
epoch [4/10] batch [80/304] time 0.219 (1.267) data 0.000 (1.067) loss 0.0446 (0.1459) lr 1.8090e-03 eta 0:43:14
epoch [4/10] batch [100/304] time 0.219 (1.055) data 0.000 (0.854) loss 0.4519 (0.1386) lr 1.8090e-03 eta 0:35:40
epoch [4/10] batch [120/304] time 0.203 (0.912) data 0.000 (0.712) loss 0.0290 (0.1344) lr 1.8090e-03 eta 0:30:31
epoch [4/10] batch [140/304] time 0.187 (0.812) data 0.000 (0.610) loss 0.0024 (0.1231) lr 1.8090e-03 eta 0:26:54
epoch [4/10] batch [160/304] time 0.188 (0.737) data 0.000 (0.534) loss 0.0041 (0.1356) lr 1.8090e-03 eta 0:24:10
epoch [4/10] batch [180/304] time 0.187 (0.678) data 0.000 (0.474) loss 0.2040 (0.1247) lr 1.8090e-03 eta 0:22:00
epoch [4/10] batch [200/304] time 0.219 (0.632) data 0.000 (0.427) loss 0.0003 (0.1209) lr 1.8090e-03 eta 0:20:17
epoch [4/10] batch [220/304] time 0.234 (0.593) data 0.000 (0.388) loss 0.0033 (0.1373) lr 1.8090e-03 eta 0:18:51
epoch [4/10] batch [240/304] time 0.219 (0.562) data 0.000 (0.356) loss 0.0745 (0.1343) lr 1.8090e-03 eta 0:17:40
epoch [4/10] batch [260/304] time 0.187 (0.534) data 0.000 (0.329) loss 0.1892 (0.1529) lr 1.8090e-03 eta 0:16:38
epoch [4/10] batch [280/304] time 0.219 (0.511) data 0.000 (0.305) loss 0.0006 (0.1519) lr 1.8090e-03 eta 0:15:44
epoch [4/10] batch [300/304] time 0.209 (0.491) data 0.000 (0.285) loss 0.0485 (0.1637) lr 1.8090e-03 eta 0:14:57
epoch [5/10] batch [20/304] time 0.172 (4.704) data 0.000 (4.501) loss 0.0568 (0.1745) lr 1.5878e-03 eta 2:21:25
epoch [5/10] batch [40/304] time 0.187 (2.444) data 0.000 (2.251) loss 0.0027 (0.1207) lr 1.5878e-03 eta 1:12:40
epoch [5/10] batch [60/304] time 0.187 (1.692) data 0.016 (1.501) loss 0.6523 (0.1202) lr 1.5878e-03 eta 0:49:44
epoch [5/10] batch [80/304] time 0.203 (1.317) data 0.000 (1.126) loss 0.0566 (0.1094) lr 1.5878e-03 eta 0:38:16
epoch [5/10] batch [100/304] time 0.172 (1.092) data 0.000 (0.901) loss 0.0084 (0.1098) lr 1.5878e-03 eta 0:31:22
epoch [5/10] batch [120/304] time 0.203 (0.944) data 0.000 (0.751) loss 0.1025 (0.1436) lr 1.5878e-03 eta 0:26:48
epoch [5/10] batch [140/304] time 0.172 (0.837) data 0.000 (0.643) loss 0.0013 (0.1338) lr 1.5878e-03 eta 0:23:30
epoch [5/10] batch [160/304] time 0.187 (0.758) data 0.000 (0.563) loss 0.0850 (0.1374) lr 1.5878e-03 eta 0:21:01
epoch [5/10] batch [180/304] time 0.203 (0.697) data 0.016 (0.501) loss 0.0113 (0.1355) lr 1.5878e-03 eta 0:19:06
epoch [5/10] batch [200/304] time 0.188 (0.649) data 0.000 (0.450) loss 0.0005 (0.1651) lr 1.5878e-03 eta 0:17:33
epoch [5/10] batch [220/304] time 0.219 (0.609) data 0.000 (0.410) loss 0.0265 (0.1790) lr 1.5878e-03 eta 0:16:16
epoch [5/10] batch [240/304] time 0.203 (0.575) data 0.000 (0.375) loss 0.0508 (0.1889) lr 1.5878e-03 eta 0:15:11
epoch [5/10] batch [260/304] time 0.234 (0.548) data 0.000 (0.347) loss 0.1411 (0.1872) lr 1.5878e-03 eta 0:14:16
epoch [5/10] batch [280/304] time 0.203 (0.524) data 0.000 (0.322) loss 0.4299 (0.1791) lr 1.5878e-03 eta 0:13:28
epoch [5/10] batch [300/304] time 0.235 (0.503) data 0.000 (0.300) loss 0.0006 (0.1699) lr 1.5878e-03 eta 0:12:46
epoch [6/10] batch [20/304] time 0.250 (4.699) data 0.000 (4.487) loss 0.0930 (0.0582) lr 1.3090e-03 eta 1:57:28
epoch [6/10] batch [40/304] time 0.219 (2.455) data 0.000 (2.244) loss 0.0006 (0.1197) lr 1.3090e-03 eta 1:00:33
epoch [6/10] batch [60/304] time 0.264 (1.712) data 0.000 (1.496) loss 0.0838 (0.1251) lr 1.3090e-03 eta 0:41:39
epoch [6/10] batch [80/304] time 0.172 (1.335) data 0.000 (1.122) loss 0.1003 (0.1219) lr 1.3090e-03 eta 0:32:01
epoch [6/10] batch [100/304] time 0.187 (1.108) data 0.000 (0.897) loss 0.0531 (0.1583) lr 1.3090e-03 eta 0:26:12
epoch [6/10] batch [120/304] time 0.172 (0.957) data 0.000 (0.748) loss 0.0042 (0.1495) lr 1.3090e-03 eta 0:22:20
epoch [6/10] batch [140/304] time 0.203 (0.850) data 0.000 (0.641) loss 0.0001 (0.1415) lr 1.3090e-03 eta 0:19:32
epoch [6/10] batch [160/304] time 0.187 (0.770) data 0.000 (0.561) loss 0.0001 (0.1449) lr 1.3090e-03 eta 0:17:27
epoch [6/10] batch [180/304] time 0.215 (0.709) data 0.000 (0.499) loss 0.0021 (0.1307) lr 1.3090e-03 eta 0:15:49
epoch [6/10] batch [200/304] time 0.200 (0.658) data 0.000 (0.449) loss 0.0065 (0.1303) lr 1.3090e-03 eta 0:14:28
epoch [6/10] batch [220/304] time 0.223 (0.618) data 0.000 (0.408) loss 0.0003 (0.1281) lr 1.3090e-03 eta 0:13:22
epoch [6/10] batch [240/304] time 0.200 (0.584) data 0.001 (0.374) loss 0.0242 (0.1261) lr 1.3090e-03 eta 0:12:27
epoch [6/10] batch [260/304] time 0.204 (0.556) data 0.000 (0.345) loss 0.1185 (0.1277) lr 1.3090e-03 eta 0:11:40
epoch [6/10] batch [280/304] time 0.208 (0.532) data 0.000 (0.321) loss 0.2324 (0.1241) lr 1.3090e-03 eta 0:10:59
epoch [6/10] batch [300/304] time 0.224 (0.511) data 0.001 (0.299) loss 0.0536 (0.1289) lr 1.3090e-03 eta 0:10:23
epoch [7/10] batch [20/304] time 0.188 (4.799) data 0.000 (4.597) loss 0.0016 (0.0570) lr 1.0000e-03 eta 1:35:39
epoch [7/10] batch [40/304] time 0.188 (2.497) data 0.000 (2.299) loss 0.0021 (0.0678) lr 1.0000e-03 eta 0:48:56
epoch [7/10] batch [60/304] time 0.172 (1.729) data 0.000 (1.532) loss 0.0032 (0.0760) lr 1.0000e-03 eta 0:33:18
epoch [7/10] batch [80/304] time 0.187 (1.346) data 0.000 (1.149) loss 0.0716 (0.0893) lr 1.0000e-03 eta 0:25:28
epoch [7/10] batch [100/304] time 0.172 (1.118) data 0.000 (0.919) loss 0.0155 (0.0734) lr 1.0000e-03 eta 0:20:47
epoch [7/10] batch [120/304] time 0.266 (0.967) data 0.000 (0.766) loss 0.1558 (0.0657) lr 1.0000e-03 eta 0:17:39
epoch [7/10] batch [140/304] time 0.203 (0.858) data 0.000 (0.657) loss 0.0661 (0.0764) lr 1.0000e-03 eta 0:15:23
epoch [7/10] batch [160/304] time 0.250 (0.778) data 0.000 (0.575) loss 0.0002 (0.0809) lr 1.0000e-03 eta 0:13:41
epoch [7/10] batch [180/304] time 0.203 (0.717) data 0.000 (0.511) loss 0.0799 (0.0927) lr 1.0000e-03 eta 0:12:23
epoch [7/10] batch [200/304] time 0.203 (0.666) data 0.000 (0.460) loss 0.1437 (0.0879) lr 1.0000e-03 eta 0:11:17
epoch [7/10] batch [220/304] time 0.205 (0.625) data 0.000 (0.418) loss 0.2134 (0.1105) lr 1.0000e-03 eta 0:10:22
epoch [7/10] batch [240/304] time 0.203 (0.590) data 0.000 (0.383) loss 0.5112 (0.1089) lr 1.0000e-03 eta 0:09:35
epoch [7/10] batch [260/304] time 0.239 (0.562) data 0.000 (0.354) loss 0.0183 (0.1097) lr 1.0000e-03 eta 0:08:56
epoch [7/10] batch [280/304] time 0.211 (0.537) data 0.000 (0.329) loss 0.0014 (0.1112) lr 1.0000e-03 eta 0:08:23
epoch [7/10] batch [300/304] time 0.209 (0.516) data 0.000 (0.307) loss 0.0703 (0.1114) lr 1.0000e-03 eta 0:07:52
epoch [8/10] batch [20/304] time 0.203 (4.625) data 0.000 (4.436) loss 0.0220 (0.0444) lr 6.9098e-04 eta 1:08:45
epoch [8/10] batch [40/304] time 0.187 (2.407) data 0.000 (2.219) loss 0.0078 (0.0644) lr 6.9098e-04 eta 0:34:58
epoch [8/10] batch [60/304] time 0.187 (1.667) data 0.000 (1.479) loss 0.0007 (0.0739) lr 6.9098e-04 eta 0:23:40
epoch [8/10] batch [80/304] time 0.188 (1.300) data 0.000 (1.110) loss 0.0542 (0.0749) lr 6.9098e-04 eta 0:18:01
epoch [8/10] batch [100/304] time 0.203 (1.079) data 0.000 (0.888) loss 0.0017 (0.0866) lr 6.9098e-04 eta 0:14:36
epoch [8/10] batch [120/304] time 0.203 (0.932) data 0.000 (0.740) loss 0.2781 (0.1014) lr 6.9098e-04 eta 0:12:17
epoch [8/10] batch [140/304] time 0.203 (0.828) data 0.000 (0.634) loss 0.0001 (0.0905) lr 6.9098e-04 eta 0:10:38
epoch [8/10] batch [160/304] time 0.203 (0.749) data 0.000 (0.555) loss 0.0399 (0.0992) lr 6.9098e-04 eta 0:09:23
epoch [8/10] batch [180/304] time 0.203 (0.688) data 0.000 (0.493) loss 0.0408 (0.1015) lr 6.9098e-04 eta 0:08:23
epoch [8/10] batch [200/304] time 0.203 (0.640) data 0.000 (0.444) loss 0.3999 (0.1008) lr 6.9098e-04 eta 0:07:35
epoch [8/10] batch [220/304] time 0.187 (0.600) data 0.000 (0.404) loss 0.0006 (0.0967) lr 6.9098e-04 eta 0:06:55
epoch [8/10] batch [240/304] time 0.219 (0.569) data 0.000 (0.370) loss 0.0196 (0.0945) lr 6.9098e-04 eta 0:06:22
epoch [8/10] batch [260/304] time 0.195 (0.542) data 0.001 (0.342) loss 0.0184 (0.0970) lr 6.9098e-04 eta 0:05:53
epoch [8/10] batch [280/304] time 0.219 (0.519) data 0.000 (0.317) loss 0.0191 (0.0950) lr 6.9098e-04 eta 0:05:27
epoch [8/10] batch [300/304] time 0.203 (0.498) data 0.000 (0.296) loss 0.0509 (0.0923) lr 6.9098e-04 eta 0:05:05
epoch [9/10] batch [20/304] time 0.172 (4.888) data 0.000 (4.688) loss 0.0814 (0.0819) lr 4.1221e-04 eta 0:47:54
epoch [9/10] batch [40/304] time 0.187 (2.538) data 0.000 (2.344) loss 0.0016 (0.0697) lr 4.1221e-04 eta 0:24:01
epoch [9/10] batch [60/304] time 0.219 (1.758) data 0.000 (1.563) loss 0.0054 (0.0603) lr 4.1221e-04 eta 0:16:03
epoch [9/10] batch [80/304] time 0.187 (1.367) data 0.000 (1.172) loss 0.0552 (0.0758) lr 4.1221e-04 eta 0:12:01
epoch [9/10] batch [100/304] time 0.219 (1.133) data 0.000 (0.938) loss 0.4963 (0.0857) lr 4.1221e-04 eta 0:09:35
epoch [9/10] batch [120/304] time 0.242 (0.979) data 0.000 (0.781) loss 0.1033 (0.0812) lr 4.1221e-04 eta 0:07:57
epoch [9/10] batch [140/304] time 0.207 (0.868) data 0.000 (0.670) loss 0.0071 (0.0781) lr 4.1221e-04 eta 0:06:46
epoch [9/10] batch [160/304] time 0.205 (0.785) data 0.000 (0.586) loss 0.0973 (0.0746) lr 4.1221e-04 eta 0:05:51
epoch [9/10] batch [180/304] time 0.204 (0.721) data 0.000 (0.521) loss 0.2134 (0.0744) lr 4.1221e-04 eta 0:05:08
epoch [9/10] batch [200/304] time 0.232 (0.670) data 0.000 (0.469) loss 0.0007 (0.0768) lr 4.1221e-04 eta 0:04:33
epoch [9/10] batch [220/304] time 0.191 (0.629) data 0.000 (0.426) loss 0.0008 (0.0748) lr 4.1221e-04 eta 0:04:03
epoch [9/10] batch [240/304] time 0.203 (0.594) data 0.000 (0.391) loss 0.0027 (0.0793) lr 4.1221e-04 eta 0:03:38
epoch [9/10] batch [260/304] time 0.203 (0.564) data 0.000 (0.361) loss 0.0038 (0.0807) lr 4.1221e-04 eta 0:03:16
epoch [9/10] batch [280/304] time 0.203 (0.539) data 0.000 (0.335) loss 0.0419 (0.0796) lr 4.1221e-04 eta 0:02:56
epoch [9/10] batch [300/304] time 0.204 (0.516) data 0.000 (0.313) loss 0.0005 (0.0768) lr 4.1221e-04 eta 0:02:39
epoch [10/10] batch [20/304] time 0.156 (4.852) data 0.000 (4.648) loss 0.0000 (0.0659) lr 1.9098e-04 eta 0:22:58
epoch [10/10] batch [40/304] time 0.172 (2.525) data 0.000 (2.324) loss 0.0004 (0.0752) lr 1.9098e-04 eta 0:11:06
epoch [10/10] batch [60/304] time 0.187 (1.747) data 0.000 (1.549) loss 0.0078 (0.0937) lr 1.9098e-04 eta 0:07:06
epoch [10/10] batch [80/304] time 0.234 (1.360) data 0.000 (1.162) loss 0.0833 (0.0805) lr 1.9098e-04 eta 0:05:04
epoch [10/10] batch [100/304] time 0.203 (1.128) data 0.000 (0.930) loss 0.0023 (0.0722) lr 1.9098e-04 eta 0:03:50
epoch [10/10] batch [120/304] time 0.234 (0.974) data 0.000 (0.775) loss 0.0023 (0.0757) lr 1.9098e-04 eta 0:02:59
epoch [10/10] batch [140/304] time 0.187 (0.865) data 0.000 (0.664) loss 0.0044 (0.0683) lr 1.9098e-04 eta 0:02:21
epoch [10/10] batch [160/304] time 0.203 (0.783) data 0.000 (0.581) loss 0.0601 (0.0659) lr 1.9098e-04 eta 0:01:52
epoch [10/10] batch [180/304] time 0.203 (0.720) data 0.000 (0.517) loss 0.0103 (0.0659) lr 1.9098e-04 eta 0:01:29
epoch [10/10] batch [200/304] time 0.203 (0.668) data 0.000 (0.465) loss 0.0072 (0.0650) lr 1.9098e-04 eta 0:01:09
epoch [10/10] batch [220/304] time 0.219 (0.627) data 0.000 (0.423) loss 0.0012 (0.0657) lr 1.9098e-04 eta 0:00:52
epoch [10/10] batch [240/304] time 0.203 (0.593) data 0.000 (0.388) loss 0.0289 (0.0672) lr 1.9098e-04 eta 0:00:37
epoch [10/10] batch [260/304] time 0.188 (0.563) data 0.000 (0.358) loss 0.0019 (0.0678) lr 1.9098e-04 eta 0:00:24
epoch [10/10] batch [280/304] time 0.234 (0.538) data 0.000 (0.332) loss 0.0005 (0.0716) lr 1.9098e-04 eta 0:00:12
epoch [10/10] batch [300/304] time 0.203 (0.517) data 0.000 (0.310) loss 0.0097 (0.0699) lr 1.9098e-04 eta 0:00:02
Checkpoint saved to output/3/base2new/train_base/oxford_pets/shots_16/CoCoOp/vit_b16_c16_ep10_batch1/seed1\prompt_learner\model.pth.tar-10
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 1,881
* correct: 1,780
* accuracy: 94.6%
* error: 5.4%
* macro_f1: 94.6%
Elapsed: 0:29:42
