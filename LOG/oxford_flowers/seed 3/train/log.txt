***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/CoCoOp/vit_b16_c16_ep10_batch1_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/base2new/train_base/oxford_flowers/shots_16/CoCoOp/vit_b16_c16_ep10_batch1_ctxv1/seed3
resume: 
root: ../DATA
seed: 3
source_domains: None
target_domains: None
trainer: CoCoOp
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 1
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: ../DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_flip',)
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 10
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/CoCoOp/vit_b16_c16_ep10_batch1_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: CoCoOp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.2.2+cu118
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 ¼ÒÍ¥ÖÐÎÄ°æ
GCC version: (x86_64-win32-seh-rev0, Built by MinGW-W64 project) 8.1.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22631-SP0
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2060
Nvidia driver version: 527.54
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture=9
CurrentClockSpeed=2592
DeviceID=CPU0
Family=198
L2CacheSize=1536
L2CacheSpeed=
Manufacturer=GenuineIntel
MaxClockSpeed=2592
Name=Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz
ProcessorType=3
Revision=

Versions of relevant libraries:
[pip3] flake8==3.7.9
[pip3] numpy==1.26.0
[pip3] torch==2.2.2+cu118
[pip3] torchaudio==2.2.2+cu118
[pip3] torchvision==0.17.2+cu118
[conda] blas                      1.0                         mkl  
[conda] mkl                       2023.1.0         h6b88ed4_46357  
[conda] mkl-service               2.4.0           py310h2bbff1b_1  
[conda] mkl_fft                   1.3.8           py310h2bbff1b_0  
[conda] mkl_random                1.2.4           py310h59b6b97_0  
[conda] numpy                     1.26.0          py310h055cbcc_0  
[conda] numpy-base                1.26.0          py310h65a83cf_0  
[conda] torch                     1.13.1+cu117             pypi_0    pypi
[conda] torchaudio                0.13.1+cu117             pypi_0    pypi
[conda] torchvision               0.14.1                   pypi_0    pypi
        Pillow (9.4.0)

Loading trainer: CoCoOp
Loading dataset: OxfordFlowers
Reading split from C:\Jupyter\MyConda\Experiment\1_CoOp_\DATA\oxford_flowers\split_zhou_OxfordFlowers.json
Creating a 16-shot dataset
Creating a 4-shot dataset
Saving preprocessed few-shot data to C:\Jupyter\MyConda\Experiment\1_CoOp_\DATA\oxford_flowers\split_fewshot\shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ resize to 224x224
+ random flip
+ to torch tensor of range [0, 1]
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Aid context: "X X"
Number of aid context words (tokens): 2
Turning off gradients in both the image and the text encoder
Parameters to be updated: {'prompt_learner.ctx', 'prompt_learner.meta_net.linear1.weight', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.bias'}
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/CoCoOp/vit_b16_c16_ep10_batch1_ctxv1/seed3\tensorboard)
epoch [1/10] batch [20/816] time 1.949 (6.274) data 0.000 (4.236) loss 2.1523 (3.8334) lr 1.0000e-05 eta 14:11:10
epoch [1/10] batch [40/816] time 1.616 (4.102) data 0.000 (2.118) loss 5.1367 (3.3988) lr 1.0000e-05 eta 9:15:08
epoch [1/10] batch [60/816] time 1.939 (3.380) data 0.001 (1.412) loss 0.2988 (2.8140) lr 1.0000e-05 eta 7:36:16
epoch [1/10] batch [80/816] time 2.017 (3.010) data 0.000 (1.059) loss 2.9805 (2.5804) lr 1.0000e-05 eta 6:45:22
epoch [1/10] batch [100/816] time 1.796 (2.780) data 0.000 (0.847) loss 0.5781 (2.2230) lr 1.0000e-05 eta 6:13:30
epoch [1/10] batch [120/816] time 2.067 (2.639) data 0.000 (0.706) loss 0.7354 (2.1201) lr 1.0000e-05 eta 5:53:39
epoch [1/10] batch [140/816] time 1.893 (2.547) data 0.001 (0.605) loss 0.0923 (2.0653) lr 1.0000e-05 eta 5:40:30
epoch [1/10] batch [160/816] time 1.490 (2.475) data 0.000 (0.530) loss 4.4180 (2.0287) lr 1.0000e-05 eta 5:30:01
epoch [1/10] batch [180/816] time 1.472 (2.371) data 0.001 (0.471) loss 0.2610 (2.0631) lr 1.0000e-05 eta 5:15:21
epoch [1/10] batch [200/816] time 2.763 (2.307) data 0.000 (0.424) loss 1.1963 (2.0108) lr 1.0000e-05 eta 5:06:06
epoch [1/10] batch [220/816] time 1.930 (2.290) data 0.001 (0.385) loss 3.1777 (1.9430) lr 1.0000e-05 eta 5:03:03
epoch [1/10] batch [240/816] time 2.442 (2.279) data 0.001 (0.353) loss 0.4734 (1.8580) lr 1.0000e-05 eta 5:00:46
epoch [1/10] batch [260/816] time 2.177 (2.267) data 0.000 (0.326) loss 0.0036 (1.8462) lr 1.0000e-05 eta 4:58:26
epoch [1/10] batch [280/816] time 2.011 (2.259) data 0.000 (0.303) loss 6.0117 (1.8374) lr 1.0000e-05 eta 4:56:39
epoch [1/10] batch [300/816] time 1.578 (2.243) data 0.001 (0.283) loss 3.7363 (1.8259) lr 1.0000e-05 eta 4:53:49
epoch [1/10] batch [320/816] time 1.803 (2.230) data 0.000 (0.265) loss 1.9932 (1.7822) lr 1.0000e-05 eta 4:51:19
epoch [1/10] batch [340/816] time 1.964 (2.217) data 0.002 (0.249) loss 2.8125 (1.7560) lr 1.0000e-05 eta 4:48:59
epoch [1/10] batch [360/816] time 1.754 (2.206) data 0.000 (0.236) loss 0.2656 (1.7004) lr 1.0000e-05 eta 4:46:48
epoch [1/10] batch [380/816] time 1.850 (2.195) data 0.000 (0.223) loss 1.4180 (1.6880) lr 1.0000e-05 eta 4:44:33
epoch [1/10] batch [400/816] time 1.985 (2.185) data 0.000 (0.212) loss 4.3906 (1.7066) lr 1.0000e-05 eta 4:42:32
epoch [1/10] batch [420/816] time 1.858 (2.171) data 0.000 (0.202) loss 6.3594 (1.7006) lr 1.0000e-05 eta 4:40:04
epoch [1/10] batch [440/816] time 2.088 (2.158) data 0.000 (0.193) loss 1.3662 (1.6907) lr 1.0000e-05 eta 4:37:38
epoch [1/10] batch [460/816] time 1.444 (2.133) data 0.000 (0.184) loss 0.2795 (1.6781) lr 1.0000e-05 eta 4:33:42
epoch [1/10] batch [480/816] time 1.696 (2.110) data 0.000 (0.177) loss 0.0057 (1.6500) lr 1.0000e-05 eta 4:30:04
epoch [1/10] batch [500/816] time 2.069 (2.104) data 0.001 (0.170) loss 0.7163 (1.6092) lr 1.0000e-05 eta 4:28:36
epoch [1/10] batch [520/816] time 1.865 (2.099) data 0.001 (0.163) loss 0.2280 (1.5881) lr 1.0000e-05 eta 4:27:14
epoch [1/10] batch [540/816] time 1.657 (2.079) data 0.001 (0.157) loss 0.0004 (1.5679) lr 1.0000e-05 eta 4:24:01
epoch [1/10] batch [560/816] time 1.760 (2.068) data 0.000 (0.152) loss 0.0310 (1.5736) lr 1.0000e-05 eta 4:21:54
epoch [1/10] batch [580/816] time 1.484 (2.059) data 0.000 (0.146) loss 1.1699 (1.5717) lr 1.0000e-05 eta 4:20:08
epoch [1/10] batch [600/816] time 1.478 (2.042) data 0.000 (0.142) loss 1.2275 (1.5765) lr 1.0000e-05 eta 4:17:16
epoch [1/10] batch [620/816] time 1.684 (2.041) data 0.001 (0.137) loss 0.0100 (1.5734) lr 1.0000e-05 eta 4:16:28
epoch [1/10] batch [640/816] time 1.717 (2.039) data 0.000 (0.133) loss 0.8159 (1.5699) lr 1.0000e-05 eta 4:15:31
epoch [1/10] batch [660/816] time 1.486 (2.032) data 0.000 (0.129) loss 0.8354 (1.5632) lr 1.0000e-05 eta 4:13:59
epoch [1/10] batch [680/816] time 1.948 (2.019) data 0.000 (0.125) loss 0.0000 (1.5324) lr 1.0000e-05 eta 4:11:38
epoch [1/10] batch [700/816] time 2.169 (2.018) data 0.000 (0.121) loss 0.0543 (1.5118) lr 1.0000e-05 eta 4:10:51
epoch [1/10] batch [720/816] time 2.007 (2.017) data 0.001 (0.118) loss 1.3125 (1.4923) lr 1.0000e-05 eta 4:10:05
epoch [1/10] batch [740/816] time 1.989 (2.017) data 0.000 (0.115) loss 0.0253 (1.4749) lr 1.0000e-05 eta 4:09:22
epoch [1/10] batch [760/816] time 1.954 (2.016) data 0.000 (0.112) loss 0.2275 (1.4582) lr 1.0000e-05 eta 4:08:38
epoch [1/10] batch [780/816] time 2.392 (2.016) data 0.001 (0.109) loss 2.2031 (1.4469) lr 1.0000e-05 eta 4:07:58
epoch [1/10] batch [800/816] time 1.778 (2.015) data 0.001 (0.106) loss 0.4282 (1.4346) lr 1.0000e-05 eta 4:07:12
epoch [2/10] batch [20/816] time 1.283 (5.903) data 0.000 (4.304) loss 1.4424 (3.0679) lr 2.0000e-03 eta 12:00:36
epoch [2/10] batch [40/816] time 1.336 (3.705) data 0.000 (2.152) loss 0.5615 (2.4670) lr 2.0000e-03 eta 7:31:04
epoch [2/10] batch [60/816] time 1.798 (3.040) data 0.001 (1.435) loss 0.7549 (2.3490) lr 2.0000e-03 eta 6:09:06
epoch [2/10] batch [80/816] time 1.763 (2.756) data 0.000 (1.076) loss 0.7539 (2.2093) lr 2.0000e-03 eta 5:33:42
epoch [2/10] batch [100/816] time 1.476 (2.507) data 0.000 (0.861) loss 2.1113 (1.9603) lr 2.0000e-03 eta 5:02:41
epoch [2/10] batch [120/816] time 1.701 (2.370) data 0.001 (0.718) loss 0.3560 (1.7758) lr 2.0000e-03 eta 4:45:18
epoch [2/10] batch [140/816] time 1.476 (2.306) data 0.001 (0.615) loss 0.2104 (1.6244) lr 2.0000e-03 eta 4:36:55
epoch [2/10] batch [160/816] time 1.472 (2.211) data 0.000 (0.538) loss 0.0002 (1.6067) lr 2.0000e-03 eta 4:24:40
epoch [2/10] batch [180/816] time 2.150 (2.171) data 0.000 (0.478) loss 0.2380 (1.5296) lr 2.0000e-03 eta 4:19:16
epoch [2/10] batch [200/816] time 2.281 (2.161) data 0.001 (0.431) loss 5.4844 (1.5570) lr 2.0000e-03 eta 4:17:17
epoch [2/10] batch [220/816] time 1.889 (2.143) data 0.000 (0.392) loss 1.9570 (1.4966) lr 2.0000e-03 eta 4:14:30
epoch [2/10] batch [240/816] time 1.473 (2.091) data 0.000 (0.359) loss 2.0527 (1.4320) lr 2.0000e-03 eta 4:07:37
epoch [2/10] batch [260/816] time 2.080 (2.076) data 0.001 (0.331) loss 1.8477 (1.3974) lr 2.0000e-03 eta 4:05:08
epoch [2/10] batch [280/816] time 1.795 (2.071) data 0.000 (0.308) loss 0.0506 (1.3451) lr 2.0000e-03 eta 4:03:48
epoch [2/10] batch [300/816] time 2.164 (2.067) data 0.000 (0.287) loss 1.1699 (1.3486) lr 2.0000e-03 eta 4:02:40
epoch [2/10] batch [320/816] time 1.799 (2.063) data 0.001 (0.269) loss 0.1459 (1.2924) lr 2.0000e-03 eta 4:01:28
epoch [2/10] batch [340/816] time 2.215 (2.059) data 0.000 (0.253) loss 0.0266 (1.2897) lr 2.0000e-03 eta 4:00:19
epoch [2/10] batch [360/816] time 1.948 (2.058) data 0.000 (0.239) loss 0.1456 (1.2888) lr 2.0000e-03 eta 3:59:30
epoch [2/10] batch [380/816] time 1.905 (2.054) data 0.000 (0.227) loss 0.3816 (1.2451) lr 2.0000e-03 eta 3:58:24
epoch [2/10] batch [400/816] time 2.455 (2.055) data 0.000 (0.216) loss 0.0369 (1.2191) lr 2.0000e-03 eta 3:57:46
epoch [2/10] batch [420/816] time 1.786 (2.057) data 0.000 (0.205) loss 0.0133 (1.1908) lr 2.0000e-03 eta 3:57:22
epoch [2/10] batch [440/816] time 1.934 (2.059) data 0.000 (0.196) loss 0.0021 (1.1741) lr 2.0000e-03 eta 3:56:53
epoch [2/10] batch [460/816] time 1.784 (2.058) data 0.000 (0.187) loss 0.5259 (1.1504) lr 2.0000e-03 eta 3:56:09
epoch [2/10] batch [480/816] time 1.487 (2.038) data 0.000 (0.180) loss 0.4260 (1.1291) lr 2.0000e-03 eta 3:53:09
epoch [2/10] batch [500/816] time 1.989 (2.030) data 0.000 (0.172) loss 0.3276 (1.1190) lr 2.0000e-03 eta 3:51:30
epoch [2/10] batch [520/816] time 3.341 (2.049) data 0.000 (0.166) loss 0.0409 (1.1038) lr 2.0000e-03 eta 3:53:01
epoch [2/10] batch [540/816] time 1.911 (2.077) data 0.000 (0.160) loss 0.5112 (1.1138) lr 2.0000e-03 eta 3:55:29
epoch [2/10] batch [560/816] time 2.669 (2.087) data 0.000 (0.154) loss 0.3079 (1.0910) lr 2.0000e-03 eta 3:55:55
epoch [2/10] batch [580/816] time 1.887 (2.086) data 0.000 (0.149) loss 0.1099 (1.0701) lr 2.0000e-03 eta 3:55:12
epoch [2/10] batch [600/816] time 1.698 (2.085) data 0.001 (0.144) loss 0.0013 (1.0497) lr 2.0000e-03 eta 3:54:18
epoch [2/10] batch [620/816] time 2.036 (2.082) data 0.000 (0.139) loss 0.4668 (1.0387) lr 2.0000e-03 eta 3:53:20
epoch [2/10] batch [640/816] time 1.975 (2.081) data 0.000 (0.135) loss 2.0586 (1.0309) lr 2.0000e-03 eta 3:52:31
epoch [2/10] batch [660/816] time 1.645 (2.078) data 0.000 (0.131) loss 2.1289 (1.0146) lr 2.0000e-03 eta 3:51:31
epoch [2/10] batch [680/816] time 2.278 (2.076) data 0.000 (0.127) loss 0.5400 (1.0134) lr 2.0000e-03 eta 3:50:36
epoch [2/10] batch [700/816] time 2.269 (2.074) data 0.000 (0.123) loss 0.0905 (1.0065) lr 2.0000e-03 eta 3:49:40
epoch [2/10] batch [720/816] time 2.060 (2.076) data 0.000 (0.120) loss 0.0092 (0.9964) lr 2.0000e-03 eta 3:49:14
epoch [2/10] batch [740/816] time 2.075 (2.076) data 0.000 (0.117) loss 0.0494 (0.9879) lr 2.0000e-03 eta 3:48:27
epoch [2/10] batch [760/816] time 1.741 (2.077) data 0.000 (0.114) loss 0.8315 (0.9769) lr 2.0000e-03 eta 3:47:53
epoch [2/10] batch [780/816] time 1.674 (2.066) data 0.000 (0.111) loss 1.1748 (0.9682) lr 2.0000e-03 eta 3:46:02
epoch [2/10] batch [800/816] time 1.850 (2.062) data 0.001 (0.108) loss 0.1519 (0.9549) lr 2.0000e-03 eta 3:44:56
epoch [3/10] batch [20/816] time 1.758 (6.073) data 0.000 (4.374) loss 0.0469 (0.4553) lr 1.9511e-03 eta 10:58:41
epoch [3/10] batch [40/816] time 1.773 (4.005) data 0.000 (2.187) loss 4.0156 (0.4795) lr 1.9511e-03 eta 7:13:05
epoch [3/10] batch [60/816] time 1.991 (3.319) data 0.000 (1.458) loss 1.1680 (0.5738) lr 1.9511e-03 eta 5:57:46
epoch [3/10] batch [80/816] time 1.973 (2.986) data 0.001 (1.094) loss 0.2192 (0.5576) lr 1.9511e-03 eta 5:20:53
epoch [3/10] batch [100/816] time 1.976 (2.785) data 0.001 (0.875) loss 0.0116 (0.6098) lr 1.9511e-03 eta 4:58:24
epoch [3/10] batch [120/816] time 2.666 (2.661) data 0.001 (0.729) loss 0.1379 (0.5799) lr 1.9511e-03 eta 4:44:10
epoch [3/10] batch [140/816] time 1.936 (2.559) data 0.001 (0.625) loss 0.0233 (0.6336) lr 1.9511e-03 eta 4:32:24
epoch [3/10] batch [160/816] time 1.731 (2.484) data 0.000 (0.547) loss 0.8691 (0.6175) lr 1.9511e-03 eta 4:23:39
epoch [3/10] batch [180/816] time 2.072 (2.426) data 0.001 (0.486) loss 0.0190 (0.6264) lr 1.9511e-03 eta 4:16:40
epoch [3/10] batch [200/816] time 2.001 (2.390) data 0.000 (0.438) loss 0.0008 (0.5945) lr 1.9511e-03 eta 4:12:03
epoch [3/10] batch [220/816] time 2.171 (2.362) data 0.001 (0.398) loss 0.3406 (0.5795) lr 1.9511e-03 eta 4:08:21
epoch [3/10] batch [240/816] time 1.669 (2.297) data 0.000 (0.365) loss 2.4121 (0.5618) lr 1.9511e-03 eta 4:00:45
epoch [3/10] batch [260/816] time 3.032 (2.262) data 0.000 (0.337) loss 0.0910 (0.5892) lr 1.9511e-03 eta 3:56:19
epoch [3/10] batch [280/816] time 2.528 (2.253) data 0.000 (0.313) loss 0.0315 (0.5570) lr 1.9511e-03 eta 3:54:37
epoch [3/10] batch [300/816] time 1.735 (2.240) data 0.000 (0.292) loss 0.3000 (0.5416) lr 1.9511e-03 eta 3:52:29
epoch [3/10] batch [320/816] time 1.817 (2.227) data 0.000 (0.274) loss 0.0009 (0.5269) lr 1.9511e-03 eta 3:50:26
epoch [3/10] batch [340/816] time 1.922 (2.220) data 0.001 (0.258) loss 0.8735 (0.5194) lr 1.9511e-03 eta 3:48:58
epoch [3/10] batch [360/816] time 2.193 (2.207) data 0.001 (0.243) loss 0.0123 (0.5173) lr 1.9511e-03 eta 3:46:55
epoch [3/10] batch [380/816] time 2.011 (2.196) data 0.000 (0.231) loss 3.1602 (0.5167) lr 1.9511e-03 eta 3:44:59
epoch [3/10] batch [400/816] time 1.928 (2.185) data 0.000 (0.219) loss 0.6870 (0.5258) lr 1.9511e-03 eta 3:43:07
epoch [3/10] batch [420/816] time 1.971 (2.175) data 0.000 (0.209) loss 0.1544 (0.5112) lr 1.9511e-03 eta 3:41:22
epoch [3/10] batch [440/816] time 1.848 (2.166) data 0.001 (0.199) loss 0.2213 (0.5039) lr 1.9511e-03 eta 3:39:44
epoch [3/10] batch [460/816] time 2.059 (2.158) data 0.000 (0.190) loss 0.0004 (0.4978) lr 1.9511e-03 eta 3:38:13
epoch [3/10] batch [480/816] time 1.921 (2.150) data 0.001 (0.183) loss 0.0364 (0.4831) lr 1.9511e-03 eta 3:36:43
epoch [3/10] batch [500/816] time 1.929 (2.142) data 0.001 (0.175) loss 0.0290 (0.4784) lr 1.9511e-03 eta 3:35:13
epoch [3/10] batch [520/816] time 1.298 (2.129) data 0.000 (0.169) loss 0.0032 (0.4665) lr 1.9511e-03 eta 3:33:12
epoch [3/10] batch [540/816] time 1.748 (2.106) data 0.001 (0.162) loss 0.0138 (0.4569) lr 1.9511e-03 eta 3:30:13
epoch [3/10] batch [560/816] time 1.907 (2.095) data 0.000 (0.157) loss 0.5615 (0.4536) lr 1.9511e-03 eta 3:28:21
epoch [3/10] batch [580/816] time 1.869 (2.087) data 0.000 (0.151) loss 0.0024 (0.4430) lr 1.9511e-03 eta 3:26:54
epoch [3/10] batch [600/816] time 1.465 (2.069) data 0.000 (0.146) loss 1.0166 (0.4336) lr 1.9511e-03 eta 3:24:27
epoch [3/10] batch [620/816] time 1.841 (2.057) data 0.000 (0.141) loss 0.0032 (0.4304) lr 1.9511e-03 eta 3:22:35
epoch [3/10] batch [640/816] time 1.900 (2.052) data 0.000 (0.137) loss 1.9805 (0.4255) lr 1.9511e-03 eta 3:21:23
epoch [3/10] batch [660/816] time 1.837 (2.048) data 0.000 (0.133) loss 0.0507 (0.4298) lr 1.9511e-03 eta 3:20:19
epoch [3/10] batch [680/816] time 2.000 (2.046) data 0.000 (0.129) loss 0.0223 (0.4406) lr 1.9511e-03 eta 3:19:27
epoch [3/10] batch [700/816] time 2.459 (2.046) data 0.001 (0.125) loss 1.7500 (0.4470) lr 1.9511e-03 eta 3:18:41
epoch [3/10] batch [720/816] time 1.961 (2.043) data 0.000 (0.122) loss 0.7856 (0.4488) lr 1.9511e-03 eta 3:17:47
epoch [3/10] batch [740/816] time 2.180 (2.042) data 0.000 (0.119) loss 0.0337 (0.4474) lr 1.9511e-03 eta 3:17:00
epoch [3/10] batch [760/816] time 1.969 (2.041) data 0.000 (0.115) loss 0.9497 (0.4570) lr 1.9511e-03 eta 3:16:14
epoch [3/10] batch [780/816] time 2.022 (2.040) data 0.000 (0.112) loss 0.1384 (0.4527) lr 1.9511e-03 eta 3:15:28
epoch [3/10] batch [800/816] time 1.877 (2.039) data 0.000 (0.110) loss 0.3899 (0.4460) lr 1.9511e-03 eta 3:14:39
epoch [4/10] batch [20/816] time 1.790 (6.090) data 0.000 (4.340) loss 0.0008 (0.2380) lr 1.8090e-03 eta 9:37:41
epoch [4/10] batch [40/816] time 1.787 (3.990) data 0.000 (2.170) loss 0.0137 (0.2124) lr 1.8090e-03 eta 6:17:10
epoch [4/10] batch [60/816] time 1.771 (3.159) data 0.000 (1.447) loss 0.0840 (0.2591) lr 1.8090e-03 eta 4:57:35
epoch [4/10] batch [80/816] time 1.802 (2.765) data 0.000 (1.085) loss 0.1655 (0.2390) lr 1.8090e-03 eta 4:19:29
epoch [4/10] batch [100/816] time 1.882 (2.593) data 0.001 (0.868) loss 0.3215 (0.2770) lr 1.8090e-03 eta 4:02:33
epoch [4/10] batch [120/816] time 1.807 (2.477) data 0.001 (0.724) loss 0.0934 (0.2576) lr 1.8090e-03 eta 3:50:52
epoch [4/10] batch [140/816] time 1.865 (2.403) data 0.000 (0.620) loss 0.6797 (0.2560) lr 1.8090e-03 eta 3:43:07
epoch [4/10] batch [160/816] time 2.034 (2.345) data 0.000 (0.543) loss 0.0089 (0.2532) lr 1.8090e-03 eta 3:36:56
epoch [4/10] batch [180/816] time 2.354 (2.302) data 0.001 (0.482) loss 1.1279 (0.2619) lr 1.8090e-03 eta 3:32:15
epoch [4/10] batch [200/816] time 1.950 (2.268) data 0.001 (0.434) loss 0.0282 (0.2874) lr 1.8090e-03 eta 3:28:20
epoch [4/10] batch [220/816] time 1.941 (2.242) data 0.000 (0.395) loss 0.0623 (0.2985) lr 1.8090e-03 eta 3:25:15
epoch [4/10] batch [240/816] time 1.980 (2.220) data 0.000 (0.362) loss 0.0032 (0.2929) lr 1.8090e-03 eta 3:22:30
epoch [4/10] batch [260/816] time 2.004 (2.202) data 0.000 (0.334) loss 0.3335 (0.2935) lr 1.8090e-03 eta 3:20:06
epoch [4/10] batch [280/816] time 1.900 (2.186) data 0.000 (0.310) loss 0.5391 (0.3008) lr 1.8090e-03 eta 3:17:53
epoch [4/10] batch [300/816] time 2.039 (2.172) data 0.000 (0.290) loss 0.0663 (0.2911) lr 1.8090e-03 eta 3:15:52
epoch [4/10] batch [320/816] time 1.754 (2.153) data 0.000 (0.272) loss 0.0959 (0.2857) lr 1.8090e-03 eta 3:13:28
epoch [4/10] batch [340/816] time 1.448 (2.129) data 0.000 (0.256) loss 0.0006 (0.2863) lr 1.8090e-03 eta 3:10:35
epoch [4/10] batch [360/816] time 1.464 (2.093) data 0.001 (0.241) loss 0.1223 (0.2967) lr 1.8090e-03 eta 3:06:41
epoch [4/10] batch [380/816] time 2.100 (2.079) data 0.001 (0.229) loss 0.0547 (0.2916) lr 1.8090e-03 eta 3:04:46
epoch [4/10] batch [400/816] time 2.728 (2.079) data 0.001 (0.217) loss 1.0664 (0.3018) lr 1.8090e-03 eta 3:04:01
epoch [4/10] batch [420/816] time 1.452 (2.071) data 0.001 (0.207) loss 0.0022 (0.3035) lr 1.8090e-03 eta 3:02:40
epoch [4/10] batch [440/816] time 1.755 (2.045) data 0.000 (0.198) loss 0.1218 (0.2976) lr 1.8090e-03 eta 2:59:38
epoch [4/10] batch [460/816] time 1.843 (2.039) data 0.000 (0.189) loss 0.0224 (0.2971) lr 1.8090e-03 eta 2:58:29
epoch [4/10] batch [480/816] time 1.833 (2.039) data 0.001 (0.181) loss 0.2437 (0.2950) lr 1.8090e-03 eta 2:57:48
epoch [4/10] batch [500/816] time 2.371 (2.038) data 0.000 (0.174) loss 0.5059 (0.2925) lr 1.8090e-03 eta 2:57:01
epoch [4/10] batch [520/816] time 1.820 (2.034) data 0.000 (0.167) loss 0.1472 (0.2864) lr 1.8090e-03 eta 2:55:58
epoch [4/10] batch [540/816] time 2.153 (2.031) data 0.000 (0.161) loss 0.9458 (0.2860) lr 1.8090e-03 eta 2:55:05
epoch [4/10] batch [560/816] time 1.985 (2.031) data 0.001 (0.155) loss 1.4570 (0.2844) lr 1.8090e-03 eta 2:54:21
epoch [4/10] batch [580/816] time 1.852 (2.030) data 0.000 (0.150) loss 0.0545 (0.2822) lr 1.8090e-03 eta 2:53:36
epoch [4/10] batch [600/816] time 2.335 (2.030) data 0.000 (0.145) loss 0.2725 (0.2837) lr 1.8090e-03 eta 2:52:55
epoch [4/10] batch [620/816] time 1.982 (2.028) data 0.000 (0.140) loss 0.1805 (0.2807) lr 1.8090e-03 eta 2:52:05
epoch [4/10] batch [640/816] time 1.958 (2.027) data 0.001 (0.136) loss 0.0952 (0.2827) lr 1.8090e-03 eta 2:51:21
epoch [4/10] batch [660/816] time 2.011 (2.026) data 0.001 (0.132) loss 1.3252 (0.2842) lr 1.8090e-03 eta 2:50:35
epoch [4/10] batch [680/816] time 1.800 (2.023) data 0.001 (0.128) loss 0.0180 (0.2920) lr 1.8090e-03 eta 2:49:38
epoch [4/10] batch [700/816] time 2.118 (2.021) data 0.001 (0.124) loss 0.0220 (0.2864) lr 1.8090e-03 eta 2:48:47
epoch [4/10] batch [720/816] time 2.194 (2.021) data 0.000 (0.121) loss 0.0540 (0.2835) lr 1.8090e-03 eta 2:48:10
epoch [4/10] batch [740/816] time 1.480 (2.017) data 0.000 (0.118) loss 0.1008 (0.2847) lr 1.8090e-03 eta 2:47:10
epoch [4/10] batch [760/816] time 1.466 (2.004) data 0.000 (0.115) loss 0.1390 (0.2827) lr 1.8090e-03 eta 2:45:21
epoch [4/10] batch [780/816] time 2.266 (1.999) data 0.000 (0.112) loss 0.0355 (0.2808) lr 1.8090e-03 eta 2:44:19
epoch [4/10] batch [800/816] time 1.892 (2.000) data 0.000 (0.109) loss 0.3760 (0.2755) lr 1.8090e-03 eta 2:43:42
epoch [5/10] batch [20/816] time 1.993 (6.237) data 0.000 (4.368) loss 0.0817 (0.0802) lr 1.5878e-03 eta 8:26:51
epoch [5/10] batch [40/816] time 1.992 (4.098) data 0.000 (2.184) loss 0.0180 (0.1111) lr 1.5878e-03 eta 5:31:41
epoch [5/10] batch [60/816] time 1.995 (3.391) data 0.001 (1.456) loss 0.0773 (0.1384) lr 1.5878e-03 eta 4:33:18
epoch [5/10] batch [80/816] time 2.002 (3.032) data 0.000 (1.092) loss 0.2866 (0.1375) lr 1.5878e-03 eta 4:03:24
epoch [5/10] batch [100/816] time 1.741 (2.816) data 0.000 (0.874) loss 0.2932 (0.1362) lr 1.5878e-03 eta 3:45:04
epoch [5/10] batch [120/816] time 1.999 (2.677) data 0.000 (0.728) loss 0.9536 (0.1664) lr 1.5878e-03 eta 3:33:06
epoch [5/10] batch [140/816] time 1.816 (2.577) data 0.000 (0.624) loss 0.0562 (0.1566) lr 1.5878e-03 eta 3:24:17
epoch [5/10] batch [160/816] time 1.980 (2.498) data 0.000 (0.546) loss 0.0432 (0.1441) lr 1.5878e-03 eta 3:17:11
epoch [5/10] batch [180/816] time 1.471 (2.425) data 0.000 (0.486) loss 0.1925 (0.1497) lr 1.5878e-03 eta 3:10:35
epoch [5/10] batch [200/816] time 1.746 (2.331) data 0.000 (0.437) loss 0.1221 (0.1485) lr 1.5878e-03 eta 3:02:26
epoch [5/10] batch [220/816] time 1.697 (2.262) data 0.000 (0.397) loss 0.2097 (0.1546) lr 1.5878e-03 eta 2:56:18
epoch [5/10] batch [240/816] time 1.769 (2.233) data 0.001 (0.364) loss 0.0255 (0.1530) lr 1.5878e-03 eta 2:53:19
epoch [5/10] batch [260/816] time 1.474 (2.194) data 0.000 (0.336) loss 0.7930 (0.1589) lr 1.5878e-03 eta 2:49:30
epoch [5/10] batch [280/816] time 1.777 (2.149) data 0.000 (0.312) loss 0.0001 (0.1572) lr 1.5878e-03 eta 2:45:18
epoch [5/10] batch [300/816] time 1.896 (2.138) data 0.001 (0.292) loss 0.0217 (0.1642) lr 1.5878e-03 eta 2:43:46
epoch [5/10] batch [320/816] time 1.930 (2.130) data 0.001 (0.273) loss 0.0167 (0.1692) lr 1.5878e-03 eta 2:42:27
epoch [5/10] batch [340/816] time 2.126 (2.118) data 0.001 (0.257) loss 0.0363 (0.1753) lr 1.5878e-03 eta 2:40:51
epoch [5/10] batch [360/816] time 2.006 (2.110) data 0.001 (0.243) loss 0.6289 (0.1817) lr 1.5878e-03 eta 2:39:30
epoch [5/10] batch [380/816] time 1.955 (2.104) data 0.000 (0.230) loss 0.3972 (0.1806) lr 1.5878e-03 eta 2:38:21
epoch [5/10] batch [400/816] time 1.997 (2.098) data 0.001 (0.219) loss 0.0208 (0.1846) lr 1.5878e-03 eta 2:37:11
epoch [5/10] batch [420/816] time 1.924 (2.092) data 0.001 (0.208) loss 0.0003 (0.1840) lr 1.5878e-03 eta 2:36:05
epoch [5/10] batch [440/816] time 1.939 (2.088) data 0.000 (0.199) loss 0.0000 (0.1946) lr 1.5878e-03 eta 2:35:03
epoch [5/10] batch [460/816] time 1.791 (2.084) data 0.000 (0.190) loss 0.3748 (0.1963) lr 1.5878e-03 eta 2:34:03
epoch [5/10] batch [480/816] time 2.012 (2.079) data 0.000 (0.182) loss 1.1777 (0.1976) lr 1.5878e-03 eta 2:33:01
epoch [5/10] batch [500/816] time 1.783 (2.075) data 0.001 (0.175) loss 0.3357 (0.2009) lr 1.5878e-03 eta 2:32:02
epoch [5/10] batch [520/816] time 2.302 (2.071) data 0.000 (0.168) loss 0.0611 (0.1987) lr 1.5878e-03 eta 2:31:01
epoch [5/10] batch [540/816] time 2.029 (2.066) data 0.001 (0.162) loss 0.0482 (0.1943) lr 1.5878e-03 eta 2:30:01
epoch [5/10] batch [560/816] time 2.299 (2.066) data 0.000 (0.156) loss 0.0018 (0.1912) lr 1.5878e-03 eta 2:29:17
epoch [5/10] batch [580/816] time 1.484 (2.055) data 0.000 (0.151) loss 0.0451 (0.1924) lr 1.5878e-03 eta 2:27:48
epoch [5/10] batch [600/816] time 1.621 (2.038) data 0.001 (0.146) loss 0.1372 (0.1938) lr 1.5878e-03 eta 2:25:53
epoch [5/10] batch [620/816] time 1.810 (2.042) data 0.000 (0.141) loss 0.0347 (0.1910) lr 1.5878e-03 eta 2:25:30
epoch [5/10] batch [640/816] time 2.687 (2.047) data 0.001 (0.137) loss 0.0123 (0.1887) lr 1.5878e-03 eta 2:25:10
epoch [5/10] batch [660/816] time 2.087 (2.045) data 0.000 (0.133) loss 0.3835 (0.1905) lr 1.5878e-03 eta 2:24:24
epoch [5/10] batch [680/816] time 3.209 (2.054) data 0.001 (0.129) loss 0.0648 (0.1885) lr 1.5878e-03 eta 2:24:18
epoch [5/10] batch [700/816] time 2.689 (2.054) data 0.001 (0.125) loss 0.0239 (0.1862) lr 1.5878e-03 eta 2:23:39
epoch [5/10] batch [720/816] time 1.954 (2.052) data 0.000 (0.122) loss 0.1194 (0.1890) lr 1.5878e-03 eta 2:22:49
epoch [5/10] batch [740/816] time 1.952 (2.050) data 0.001 (0.118) loss 1.0254 (0.1888) lr 1.5878e-03 eta 2:22:01
epoch [5/10] batch [760/816] time 1.947 (2.049) data 0.001 (0.115) loss 0.0180 (0.1927) lr 1.5878e-03 eta 2:21:15
epoch [5/10] batch [780/816] time 1.909 (2.048) data 0.000 (0.112) loss 0.0033 (0.1948) lr 1.5878e-03 eta 2:20:29
epoch [5/10] batch [800/816] time 1.945 (2.048) data 0.000 (0.110) loss 0.0271 (0.1981) lr 1.5878e-03 eta 2:19:46
epoch [6/10] batch [20/816] time 1.768 (5.774) data 0.001 (4.191) loss 0.0689 (0.0929) lr 1.3090e-03 eta 6:30:42
epoch [6/10] batch [40/816] time 2.105 (3.789) data 0.000 (2.095) loss 0.1131 (0.1257) lr 1.3090e-03 eta 4:15:06
epoch [6/10] batch [60/816] time 1.758 (3.141) data 0.000 (1.397) loss 0.4604 (0.1421) lr 1.3090e-03 eta 3:30:25
epoch [6/10] batch [80/816] time 1.455 (2.733) data 0.001 (1.048) loss 0.3633 (0.1375) lr 1.3090e-03 eta 3:02:11
epoch [6/10] batch [100/816] time 2.148 (2.511) data 0.001 (0.838) loss 0.0105 (0.1281) lr 1.3090e-03 eta 2:46:33
epoch [6/10] batch [120/816] time 1.894 (2.433) data 0.001 (0.699) loss 0.4324 (0.1250) lr 1.3090e-03 eta 2:40:33
epoch [6/10] batch [140/816] time 2.091 (2.376) data 0.000 (0.599) loss 0.0696 (0.1192) lr 1.3090e-03 eta 2:36:01
epoch [6/10] batch [160/816] time 3.136 (2.338) data 0.001 (0.524) loss 0.0541 (0.1098) lr 1.3090e-03 eta 2:32:44
epoch [6/10] batch [180/816] time 2.362 (2.337) data 0.000 (0.466) loss 0.1135 (0.1174) lr 1.3090e-03 eta 2:31:54
epoch [6/10] batch [200/816] time 1.800 (2.302) data 0.000 (0.419) loss 0.0520 (0.1208) lr 1.3090e-03 eta 2:28:50
epoch [6/10] batch [220/816] time 2.100 (2.278) data 0.000 (0.381) loss 0.0066 (0.1245) lr 1.3090e-03 eta 2:26:33
epoch [6/10] batch [240/816] time 2.032 (2.257) data 0.000 (0.350) loss 0.0811 (0.1288) lr 1.3090e-03 eta 2:24:27
epoch [6/10] batch [260/816] time 2.081 (2.239) data 0.001 (0.323) loss 0.5737 (0.1303) lr 1.3090e-03 eta 2:22:34
epoch [6/10] batch [280/816] time 2.263 (2.223) data 0.001 (0.300) loss 0.3184 (0.1259) lr 1.3090e-03 eta 2:20:48
epoch [6/10] batch [300/816] time 2.075 (2.207) data 0.001 (0.280) loss 0.0019 (0.1225) lr 1.3090e-03 eta 2:19:04
epoch [6/10] batch [320/816] time 1.700 (2.199) data 0.000 (0.262) loss 0.9116 (0.1271) lr 1.3090e-03 eta 2:17:47
epoch [6/10] batch [340/816] time 1.935 (2.189) data 0.000 (0.247) loss 0.0371 (0.1261) lr 1.3090e-03 eta 2:16:26
epoch [6/10] batch [360/816] time 1.750 (2.183) data 0.000 (0.233) loss 0.4456 (0.1267) lr 1.3090e-03 eta 2:15:20
epoch [6/10] batch [380/816] time 1.721 (2.176) data 0.000 (0.221) loss 0.0007 (0.1354) lr 1.3090e-03 eta 2:14:10
epoch [6/10] batch [400/816] time 1.496 (2.148) data 0.000 (0.210) loss 0.0045 (0.1354) lr 1.3090e-03 eta 2:11:45
epoch [6/10] batch [420/816] time 1.875 (2.131) data 0.000 (0.200) loss 0.0725 (0.1323) lr 1.3090e-03 eta 2:09:58
epoch [6/10] batch [440/816] time 1.503 (2.120) data 0.000 (0.191) loss 0.0417 (0.1364) lr 1.3090e-03 eta 2:08:38
epoch [6/10] batch [460/816] time 1.469 (2.095) data 0.000 (0.183) loss 0.2454 (0.1334) lr 1.3090e-03 eta 2:06:23
epoch [6/10] batch [480/816] time 1.814 (2.084) data 0.000 (0.175) loss 0.2168 (0.1366) lr 1.3090e-03 eta 2:05:03
epoch [6/10] batch [500/816] time 1.792 (2.079) data 0.001 (0.168) loss 0.1498 (0.1359) lr 1.3090e-03 eta 2:04:03
epoch [6/10] batch [520/816] time 1.651 (2.068) data 0.000 (0.162) loss 0.0547 (0.1381) lr 1.3090e-03 eta 2:02:43
epoch [6/10] batch [540/816] time 1.817 (2.050) data 0.001 (0.156) loss 1.8730 (0.1397) lr 1.3090e-03 eta 2:00:55
epoch [6/10] batch [560/816] time 2.026 (2.056) data 0.000 (0.150) loss 0.0729 (0.1366) lr 1.3090e-03 eta 2:00:35
epoch [6/10] batch [580/816] time 1.987 (2.055) data 0.000 (0.145) loss 0.0255 (0.1373) lr 1.3090e-03 eta 1:59:52
epoch [6/10] batch [600/816] time 1.990 (2.054) data 0.001 (0.140) loss 0.0061 (0.1345) lr 1.3090e-03 eta 1:59:06
epoch [6/10] batch [620/816] time 1.893 (2.052) data 0.000 (0.136) loss 0.0400 (0.1353) lr 1.3090e-03 eta 1:58:20
epoch [6/10] batch [640/816] time 2.049 (2.052) data 0.001 (0.131) loss 0.0591 (0.1322) lr 1.3090e-03 eta 1:57:37
epoch [6/10] batch [660/816] time 2.266 (2.051) data 0.000 (0.127) loss 0.2468 (0.1314) lr 1.3090e-03 eta 1:56:53
epoch [6/10] batch [680/816] time 1.682 (2.049) data 0.000 (0.124) loss 0.0424 (0.1362) lr 1.3090e-03 eta 1:56:06
epoch [6/10] batch [700/816] time 2.211 (2.050) data 0.000 (0.120) loss 0.0765 (0.1366) lr 1.3090e-03 eta 1:55:27
epoch [6/10] batch [720/816] time 2.039 (2.053) data 0.000 (0.117) loss 0.0091 (0.1379) lr 1.3090e-03 eta 1:54:57
epoch [6/10] batch [740/816] time 1.925 (2.053) data 0.000 (0.114) loss 0.0004 (0.1376) lr 1.3090e-03 eta 1:54:16
epoch [6/10] batch [760/816] time 1.481 (2.048) data 0.000 (0.111) loss 0.0005 (0.1372) lr 1.3090e-03 eta 1:53:20
epoch [6/10] batch [780/816] time 1.472 (2.035) data 0.000 (0.108) loss 0.0320 (0.1360) lr 1.3090e-03 eta 1:51:55
epoch [6/10] batch [800/816] time 2.051 (2.031) data 0.000 (0.105) loss 0.0015 (0.1357) lr 1.3090e-03 eta 1:51:01
epoch [7/10] batch [20/816] time 2.007 (6.111) data 0.000 (4.402) loss 0.0002 (0.0596) lr 1.0000e-03 eta 5:30:24
epoch [7/10] batch [40/816] time 1.979 (4.040) data 0.001 (2.201) loss 0.0268 (0.1434) lr 1.0000e-03 eta 3:37:04
epoch [7/10] batch [60/816] time 2.230 (3.347) data 0.000 (1.467) loss 0.1396 (0.1266) lr 1.0000e-03 eta 2:58:42
epoch [7/10] batch [80/816] time 2.140 (3.000) data 0.000 (1.101) loss 0.3350 (0.1519) lr 1.0000e-03 eta 2:39:10
epoch [7/10] batch [100/816] time 2.042 (2.796) data 0.000 (0.881) loss 0.0115 (0.1316) lr 1.0000e-03 eta 2:27:25
epoch [7/10] batch [120/816] time 1.916 (2.654) data 0.000 (0.734) loss 0.0109 (0.1163) lr 1.0000e-03 eta 2:19:04
epoch [7/10] batch [140/816] time 1.987 (2.562) data 0.000 (0.629) loss 0.0964 (0.1260) lr 1.0000e-03 eta 2:13:22
epoch [7/10] batch [160/816] time 1.807 (2.491) data 0.000 (0.550) loss 0.5015 (0.1278) lr 1.0000e-03 eta 2:08:53
epoch [7/10] batch [180/816] time 2.086 (2.441) data 0.000 (0.489) loss 0.1737 (0.1223) lr 1.0000e-03 eta 2:05:28
epoch [7/10] batch [200/816] time 2.742 (2.410) data 0.000 (0.440) loss 0.0072 (0.1182) lr 1.0000e-03 eta 2:03:04
epoch [7/10] batch [220/816] time 1.738 (2.370) data 0.000 (0.400) loss 0.0112 (0.1154) lr 1.0000e-03 eta 2:00:13
epoch [7/10] batch [240/816] time 1.833 (2.361) data 0.000 (0.367) loss 0.0016 (0.1113) lr 1.0000e-03 eta 1:58:58
epoch [7/10] batch [260/816] time 1.870 (2.337) data 0.001 (0.339) loss 0.0035 (0.1103) lr 1.0000e-03 eta 1:57:00
epoch [7/10] batch [280/816] time 1.751 (2.323) data 0.000 (0.315) loss 0.0000 (0.1067) lr 1.0000e-03 eta 1:55:32
epoch [7/10] batch [300/816] time 1.947 (2.308) data 0.000 (0.294) loss 0.0018 (0.1021) lr 1.0000e-03 eta 1:53:59
epoch [7/10] batch [320/816] time 1.997 (2.290) data 0.001 (0.275) loss 0.0101 (0.1005) lr 1.0000e-03 eta 1:52:21
epoch [7/10] batch [340/816] time 1.654 (2.274) data 0.000 (0.259) loss 0.0940 (0.1005) lr 1.0000e-03 eta 1:50:49
epoch [7/10] batch [360/816] time 1.914 (2.259) data 0.000 (0.245) loss 0.4768 (0.1030) lr 1.0000e-03 eta 1:49:21
epoch [7/10] batch [380/816] time 1.862 (2.247) data 0.000 (0.232) loss 0.1680 (0.1005) lr 1.0000e-03 eta 1:47:59
epoch [7/10] batch [400/816] time 2.096 (2.235) data 0.000 (0.220) loss 0.0038 (0.1005) lr 1.0000e-03 eta 1:46:41
epoch [7/10] batch [420/816] time 2.188 (2.223) data 0.000 (0.210) loss 0.0018 (0.1042) lr 1.0000e-03 eta 1:45:22
epoch [7/10] batch [440/816] time 2.056 (2.213) data 0.000 (0.200) loss 0.0607 (0.1061) lr 1.0000e-03 eta 1:44:09
epoch [7/10] batch [460/816] time 1.870 (2.201) data 0.000 (0.192) loss 0.0262 (0.1076) lr 1.0000e-03 eta 1:42:52
epoch [7/10] batch [480/816] time 1.891 (2.196) data 0.000 (0.184) loss 0.0720 (0.1070) lr 1.0000e-03 eta 1:41:52
epoch [7/10] batch [500/816] time 1.853 (2.188) data 0.001 (0.176) loss 0.0042 (0.1046) lr 1.0000e-03 eta 1:40:46
epoch [7/10] batch [520/816] time 1.479 (2.163) data 0.000 (0.170) loss 0.0084 (0.1036) lr 1.0000e-03 eta 1:38:56
epoch [7/10] batch [540/816] time 2.701 (2.143) data 0.000 (0.163) loss 0.4531 (0.1022) lr 1.0000e-03 eta 1:37:16
epoch [7/10] batch [560/816] time 2.885 (2.143) data 0.001 (0.157) loss 0.0210 (0.0998) lr 1.0000e-03 eta 1:36:35
epoch [7/10] batch [580/816] time 1.922 (2.139) data 0.001 (0.152) loss 0.0544 (0.0976) lr 1.0000e-03 eta 1:35:41
epoch [7/10] batch [600/816] time 1.717 (2.138) data 0.000 (0.147) loss 0.0390 (0.0961) lr 1.0000e-03 eta 1:34:55
epoch [7/10] batch [620/816] time 2.242 (2.138) data 0.000 (0.142) loss 0.0518 (0.0989) lr 1.0000e-03 eta 1:34:13
epoch [7/10] batch [640/816] time 2.542 (2.138) data 0.000 (0.138) loss 0.0009 (0.0987) lr 1.0000e-03 eta 1:33:29
epoch [7/10] batch [660/816] time 2.009 (2.133) data 0.001 (0.134) loss 0.1754 (0.0970) lr 1.0000e-03 eta 1:32:35
epoch [7/10] batch [680/816] time 2.222 (2.130) data 0.000 (0.130) loss 0.0787 (0.0984) lr 1.0000e-03 eta 1:31:44
epoch [7/10] batch [700/816] time 1.836 (2.127) data 0.000 (0.126) loss 0.2722 (0.1016) lr 1.0000e-03 eta 1:30:52
epoch [7/10] batch [720/816] time 2.611 (2.125) data 0.001 (0.123) loss 1.2725 (0.1030) lr 1.0000e-03 eta 1:30:05
epoch [7/10] batch [740/816] time 2.195 (2.121) data 0.001 (0.119) loss 0.1118 (0.1025) lr 1.0000e-03 eta 1:29:13
epoch [7/10] batch [760/816] time 2.309 (2.118) data 0.000 (0.116) loss 0.0224 (0.1030) lr 1.0000e-03 eta 1:28:23
epoch [7/10] batch [780/816] time 1.781 (2.116) data 0.001 (0.113) loss 0.7881 (0.1041) lr 1.0000e-03 eta 1:27:35
epoch [7/10] batch [800/816] time 2.618 (2.118) data 0.001 (0.110) loss 0.0485 (0.1041) lr 1.0000e-03 eta 1:26:57
epoch [8/10] batch [20/816] time 1.666 (5.706) data 0.000 (4.036) loss 0.0106 (0.2201) lr 6.9098e-04 eta 3:50:54
epoch [8/10] batch [40/816] time 1.694 (3.720) data 0.001 (2.018) loss 0.1136 (0.1337) lr 6.9098e-04 eta 2:29:17
epoch [8/10] batch [60/816] time 1.773 (3.102) data 0.000 (1.345) loss 0.0189 (0.1129) lr 6.9098e-04 eta 2:03:27
epoch [8/10] batch [80/816] time 1.880 (2.796) data 0.000 (1.009) loss 0.0238 (0.1219) lr 6.9098e-04 eta 1:50:21
epoch [8/10] batch [100/816] time 1.963 (2.628) data 0.000 (0.807) loss 0.0421 (0.1078) lr 6.9098e-04 eta 1:42:51
epoch [8/10] batch [120/816] time 2.056 (2.521) data 0.001 (0.673) loss 0.2111 (0.0953) lr 6.9098e-04 eta 1:37:50
epoch [8/10] batch [140/816] time 1.920 (2.443) data 0.000 (0.577) loss 0.1818 (0.0939) lr 6.9098e-04 eta 1:33:58
epoch [8/10] batch [160/816] time 1.972 (2.385) data 0.000 (0.505) loss 0.0014 (0.0878) lr 6.9098e-04 eta 1:30:57
epoch [8/10] batch [180/816] time 2.005 (2.339) data 0.000 (0.449) loss 0.0001 (0.0900) lr 6.9098e-04 eta 1:28:24
epoch [8/10] batch [200/816] time 1.848 (2.301) data 0.000 (0.404) loss 0.0006 (0.0856) lr 6.9098e-04 eta 1:26:13
epoch [8/10] batch [220/816] time 1.978 (2.275) data 0.001 (0.367) loss 0.0491 (0.0806) lr 6.9098e-04 eta 1:24:29
epoch [8/10] batch [240/816] time 2.276 (2.251) data 0.000 (0.337) loss 0.0738 (0.0781) lr 6.9098e-04 eta 1:22:49
epoch [8/10] batch [260/816] time 1.477 (2.224) data 0.001 (0.311) loss 0.0969 (0.0747) lr 6.9098e-04 eta 1:21:05
epoch [8/10] batch [280/816] time 1.487 (2.175) data 0.001 (0.289) loss 0.0163 (0.0754) lr 6.9098e-04 eta 1:18:35
epoch [8/10] batch [300/816] time 1.710 (2.146) data 0.001 (0.269) loss 0.0202 (0.0803) lr 6.9098e-04 eta 1:16:50
epoch [8/10] batch [320/816] time 1.972 (2.138) data 0.000 (0.253) loss 0.0006 (0.0830) lr 6.9098e-04 eta 1:15:48
epoch [8/10] batch [340/816] time 1.752 (2.120) data 0.000 (0.238) loss 0.0001 (0.0847) lr 6.9098e-04 eta 1:14:28
epoch [8/10] batch [360/816] time 1.481 (2.086) data 0.000 (0.224) loss 0.0040 (0.0826) lr 6.9098e-04 eta 1:12:35
epoch [8/10] batch [380/816] time 1.879 (2.081) data 0.000 (0.213) loss 0.0032 (0.0809) lr 6.9098e-04 eta 1:11:43
epoch [8/10] batch [400/816] time 1.473 (2.061) data 0.001 (0.202) loss 0.0325 (0.0796) lr 6.9098e-04 eta 1:10:19
epoch [8/10] batch [420/816] time 1.769 (2.033) data 0.000 (0.192) loss 0.0092 (0.0793) lr 6.9098e-04 eta 1:08:43
epoch [8/10] batch [440/816] time 1.725 (2.025) data 0.001 (0.184) loss 0.0075 (0.0773) lr 6.9098e-04 eta 1:07:46
epoch [8/10] batch [460/816] time 1.890 (2.025) data 0.000 (0.176) loss 0.2095 (0.0777) lr 6.9098e-04 eta 1:07:05
epoch [8/10] batch [480/816] time 1.474 (2.017) data 0.000 (0.168) loss 0.0091 (0.0782) lr 6.9098e-04 eta 1:06:09
epoch [8/10] batch [500/816] time 1.455 (1.997) data 0.000 (0.162) loss 0.3064 (0.0788) lr 6.9098e-04 eta 1:04:50
epoch [8/10] batch [520/816] time 2.519 (1.994) data 0.000 (0.155) loss 0.0615 (0.0780) lr 6.9098e-04 eta 1:04:03
epoch [8/10] batch [540/816] time 1.982 (1.992) data 0.000 (0.150) loss 0.0662 (0.0803) lr 6.9098e-04 eta 1:03:21
epoch [8/10] batch [560/816] time 2.049 (1.993) data 0.000 (0.144) loss 0.0080 (0.0789) lr 6.9098e-04 eta 1:02:42
epoch [8/10] batch [580/816] time 1.966 (1.993) data 0.001 (0.139) loss 0.0124 (0.0796) lr 6.9098e-04 eta 1:02:02
epoch [8/10] batch [600/816] time 2.025 (1.993) data 0.000 (0.135) loss 0.0029 (0.0791) lr 6.9098e-04 eta 1:01:22
epoch [8/10] batch [620/816] time 2.106 (1.992) data 0.000 (0.130) loss 0.7002 (0.0793) lr 6.9098e-04 eta 1:00:42
epoch [8/10] batch [640/816] time 1.970 (1.992) data 0.001 (0.126) loss 0.0235 (0.0776) lr 6.9098e-04 eta 1:00:01
epoch [8/10] batch [660/816] time 1.964 (1.991) data 0.000 (0.123) loss 0.0282 (0.0775) lr 6.9098e-04 eta 0:59:19
epoch [8/10] batch [680/816] time 2.104 (1.991) data 0.000 (0.119) loss 0.0370 (0.0763) lr 6.9098e-04 eta 0:58:39
epoch [8/10] batch [700/816] time 2.714 (1.994) data 0.000 (0.116) loss 0.0089 (0.0763) lr 6.9098e-04 eta 0:58:04
epoch [8/10] batch [720/816] time 1.484 (1.991) data 0.000 (0.112) loss 0.0662 (0.0751) lr 6.9098e-04 eta 0:57:20
epoch [8/10] batch [740/816] time 1.466 (1.979) data 0.000 (0.109) loss 0.1158 (0.0738) lr 6.9098e-04 eta 0:56:19
epoch [8/10] batch [760/816] time 2.087 (1.971) data 0.000 (0.106) loss 0.0002 (0.0747) lr 6.9098e-04 eta 0:55:26
epoch [8/10] batch [780/816] time 1.680 (1.971) data 0.000 (0.104) loss 0.0872 (0.0744) lr 6.9098e-04 eta 0:54:47
epoch [8/10] batch [800/816] time 1.483 (1.959) data 0.001 (0.101) loss 0.7954 (0.0761) lr 6.9098e-04 eta 0:53:48
epoch [9/10] batch [20/816] time 1.912 (6.166) data 0.000 (4.419) loss 0.0220 (0.0950) lr 4.1221e-04 eta 2:45:39
epoch [9/10] batch [40/816] time 1.935 (4.006) data 0.000 (2.210) loss 0.0077 (0.0849) lr 4.1221e-04 eta 1:46:17
epoch [9/10] batch [60/816] time 1.836 (3.258) data 0.000 (1.473) loss 0.0151 (0.0906) lr 4.1221e-04 eta 1:25:21
epoch [9/10] batch [80/816] time 1.929 (2.931) data 0.000 (1.105) loss 0.0186 (0.0824) lr 4.1221e-04 eta 1:15:49
epoch [9/10] batch [100/816] time 1.926 (2.734) data 0.000 (0.884) loss 0.1663 (0.0744) lr 4.1221e-04 eta 1:09:47
epoch [9/10] batch [120/816] time 1.970 (2.599) data 0.000 (0.737) loss 0.0578 (0.0781) lr 4.1221e-04 eta 1:05:30
epoch [9/10] batch [140/816] time 1.955 (2.480) data 0.001 (0.632) loss 0.0602 (0.0771) lr 4.1221e-04 eta 1:01:40
epoch [9/10] batch [160/816] time 1.613 (2.407) data 0.001 (0.553) loss 0.0000 (0.0789) lr 4.1221e-04 eta 0:59:03
epoch [9/10] batch [180/816] time 2.057 (2.338) data 0.000 (0.491) loss 0.0056 (0.0754) lr 4.1221e-04 eta 0:56:34
epoch [9/10] batch [200/816] time 1.831 (2.287) data 0.000 (0.442) loss 1.2227 (0.0771) lr 4.1221e-04 eta 0:54:34
epoch [9/10] batch [220/816] time 1.977 (2.249) data 0.001 (0.402) loss 0.0763 (0.0767) lr 4.1221e-04 eta 0:52:55
epoch [9/10] batch [240/816] time 2.006 (2.215) data 0.001 (0.369) loss 0.0001 (0.0730) lr 4.1221e-04 eta 0:51:23
epoch [9/10] batch [260/816] time 1.865 (2.188) data 0.000 (0.340) loss 0.0025 (0.0725) lr 4.1221e-04 eta 0:50:01
epoch [9/10] batch [280/816] time 1.957 (2.171) data 0.000 (0.316) loss 0.0001 (0.0695) lr 4.1221e-04 eta 0:48:55
epoch [9/10] batch [300/816] time 2.647 (2.161) data 0.001 (0.295) loss 0.1769 (0.0702) lr 4.1221e-04 eta 0:47:57
epoch [9/10] batch [320/816] time 1.928 (2.148) data 0.000 (0.277) loss 0.0005 (0.0678) lr 4.1221e-04 eta 0:46:58
epoch [9/10] batch [340/816] time 1.929 (2.140) data 0.001 (0.260) loss 0.0000 (0.0728) lr 4.1221e-04 eta 0:46:05
epoch [9/10] batch [360/816] time 1.910 (2.132) data 0.000 (0.246) loss 0.0129 (0.0722) lr 4.1221e-04 eta 0:45:12
epoch [9/10] batch [380/816] time 2.017 (2.125) data 0.000 (0.233) loss 0.0166 (0.0716) lr 4.1221e-04 eta 0:44:20
epoch [9/10] batch [400/816] time 2.018 (2.118) data 0.001 (0.221) loss 0.0011 (0.0715) lr 4.1221e-04 eta 0:43:29
epoch [9/10] batch [420/816] time 1.633 (2.114) data 0.000 (0.211) loss 0.0169 (0.0726) lr 4.1221e-04 eta 0:42:42
epoch [9/10] batch [440/816] time 1.926 (2.109) data 0.001 (0.201) loss 0.0000 (0.0720) lr 4.1221e-04 eta 0:41:53
epoch [9/10] batch [460/816] time 1.991 (2.103) data 0.000 (0.192) loss 0.0426 (0.0718) lr 4.1221e-04 eta 0:41:05
epoch [9/10] batch [480/816] time 1.844 (2.101) data 0.000 (0.184) loss 0.0114 (0.0707) lr 4.1221e-04 eta 0:40:20
epoch [9/10] batch [500/816] time 1.744 (2.099) data 0.001 (0.177) loss 0.0058 (0.0688) lr 4.1221e-04 eta 0:39:35
epoch [9/10] batch [520/816] time 2.534 (2.098) data 0.001 (0.170) loss 0.0519 (0.0686) lr 4.1221e-04 eta 0:38:53
epoch [9/10] batch [540/816] time 1.463 (2.081) data 0.001 (0.164) loss 0.0101 (0.0684) lr 4.1221e-04 eta 0:37:52
epoch [9/10] batch [560/816] time 1.486 (2.061) data 0.000 (0.158) loss 0.0022 (0.0677) lr 4.1221e-04 eta 0:36:49
epoch [9/10] batch [580/816] time 1.804 (2.061) data 0.000 (0.153) loss 0.0303 (0.0665) lr 4.1221e-04 eta 0:36:07
epoch [9/10] batch [600/816] time 1.970 (2.058) data 0.000 (0.148) loss 0.0107 (0.0663) lr 4.1221e-04 eta 0:35:23
epoch [9/10] batch [620/816] time 1.874 (2.057) data 0.001 (0.143) loss 0.0311 (0.0659) lr 4.1221e-04 eta 0:34:41
epoch [9/10] batch [640/816] time 1.718 (2.057) data 0.000 (0.138) loss 0.1903 (0.0666) lr 4.1221e-04 eta 0:34:00
epoch [9/10] batch [660/816] time 2.577 (2.057) data 0.000 (0.134) loss 0.0293 (0.0654) lr 4.1221e-04 eta 0:33:19
epoch [9/10] batch [680/816] time 1.891 (2.054) data 0.000 (0.130) loss 0.0975 (0.0653) lr 4.1221e-04 eta 0:32:35
epoch [9/10] batch [700/816] time 1.798 (2.053) data 0.001 (0.127) loss 0.0392 (0.0654) lr 4.1221e-04 eta 0:31:53
epoch [9/10] batch [720/816] time 2.152 (2.053) data 0.001 (0.123) loss 0.0614 (0.0643) lr 4.1221e-04 eta 0:31:12
epoch [9/10] batch [740/816] time 2.064 (2.051) data 0.000 (0.120) loss 0.2629 (0.0638) lr 4.1221e-04 eta 0:30:29
epoch [9/10] batch [760/816] time 2.032 (2.049) data 0.001 (0.117) loss 0.0353 (0.0634) lr 4.1221e-04 eta 0:29:47
epoch [9/10] batch [780/816] time 1.965 (2.047) data 0.000 (0.114) loss 0.0117 (0.0626) lr 4.1221e-04 eta 0:29:03
epoch [9/10] batch [800/816] time 2.404 (2.047) data 0.000 (0.111) loss 0.0060 (0.0647) lr 4.1221e-04 eta 0:28:23
epoch [10/10] batch [20/816] time 1.663 (6.076) data 0.001 (4.396) loss 0.0347 (0.0881) lr 1.9098e-04 eta 1:20:36
epoch [10/10] batch [40/816] time 1.693 (3.881) data 0.000 (2.198) loss 0.0088 (0.0676) lr 1.9098e-04 eta 0:50:11
epoch [10/10] batch [60/816] time 1.681 (3.149) data 0.001 (1.466) loss 0.0000 (0.0528) lr 1.9098e-04 eta 0:39:40
epoch [10/10] batch [80/816] time 1.723 (2.790) data 0.001 (1.099) loss 0.0986 (0.0492) lr 1.9098e-04 eta 0:34:13
epoch [10/10] batch [100/816] time 1.865 (2.604) data 0.000 (0.880) loss 0.0043 (0.0441) lr 1.9098e-04 eta 0:31:04
epoch [10/10] batch [120/816] time 1.990 (2.501) data 0.000 (0.733) loss 0.0789 (0.0430) lr 1.9098e-04 eta 0:29:00
epoch [10/10] batch [140/816] time 1.938 (2.426) data 0.000 (0.628) loss 0.0019 (0.0421) lr 1.9098e-04 eta 0:27:19
epoch [10/10] batch [160/816] time 1.747 (2.367) data 0.000 (0.550) loss 0.0107 (0.0438) lr 1.9098e-04 eta 0:25:52
epoch [10/10] batch [180/816] time 1.959 (2.317) data 0.000 (0.489) loss 0.1395 (0.0435) lr 1.9098e-04 eta 0:24:33
epoch [10/10] batch [200/816] time 1.839 (2.277) data 0.000 (0.440) loss 0.0244 (0.0435) lr 1.9098e-04 eta 0:23:22
epoch [10/10] batch [220/816] time 2.056 (2.246) data 0.000 (0.400) loss 0.0558 (0.0421) lr 1.9098e-04 eta 0:22:18
epoch [10/10] batch [240/816] time 1.784 (2.220) data 0.000 (0.367) loss 0.0287 (0.0423) lr 1.9098e-04 eta 0:21:18
epoch [10/10] batch [260/816] time 1.696 (2.190) data 0.000 (0.338) loss 0.1394 (0.0443) lr 1.9098e-04 eta 0:20:17
epoch [10/10] batch [280/816] time 1.462 (2.163) data 0.001 (0.314) loss 0.0114 (0.0463) lr 1.9098e-04 eta 0:19:19
epoch [10/10] batch [300/816] time 1.752 (2.119) data 0.000 (0.293) loss 0.0284 (0.0467) lr 1.9098e-04 eta 0:18:13
epoch [10/10] batch [320/816] time 1.936 (2.091) data 0.001 (0.275) loss 0.0327 (0.0459) lr 1.9098e-04 eta 0:17:17
epoch [10/10] batch [340/816] time 1.782 (2.075) data 0.000 (0.259) loss 0.0024 (0.0458) lr 1.9098e-04 eta 0:16:27
epoch [10/10] batch [360/816] time 1.307 (2.050) data 0.000 (0.245) loss 0.0014 (0.0521) lr 1.9098e-04 eta 0:15:34
epoch [10/10] batch [380/816] time 1.753 (2.025) data 0.000 (0.232) loss 0.0179 (0.0576) lr 1.9098e-04 eta 0:14:43
epoch [10/10] batch [400/816] time 1.868 (2.016) data 0.000 (0.220) loss 0.0142 (0.0578) lr 1.9098e-04 eta 0:13:58
epoch [10/10] batch [420/816] time 2.316 (2.021) data 0.000 (0.210) loss 0.0000 (0.0573) lr 1.9098e-04 eta 0:13:20
epoch [10/10] batch [440/816] time 1.901 (2.021) data 0.001 (0.200) loss 0.0630 (0.0565) lr 1.9098e-04 eta 0:12:39
epoch [10/10] batch [460/816] time 2.138 (2.019) data 0.000 (0.191) loss 0.0474 (0.0554) lr 1.9098e-04 eta 0:11:58
epoch [10/10] batch [480/816] time 1.980 (2.017) data 0.000 (0.183) loss 0.0135 (0.0542) lr 1.9098e-04 eta 0:11:17
epoch [10/10] batch [500/816] time 2.050 (2.017) data 0.001 (0.176) loss 0.1588 (0.0535) lr 1.9098e-04 eta 0:10:37
epoch [10/10] batch [520/816] time 1.988 (2.015) data 0.000 (0.169) loss 0.0409 (0.0534) lr 1.9098e-04 eta 0:09:56
epoch [10/10] batch [540/816] time 1.997 (2.012) data 0.001 (0.163) loss 0.0117 (0.0525) lr 1.9098e-04 eta 0:09:15
epoch [10/10] batch [560/816] time 1.678 (2.011) data 0.000 (0.157) loss 0.0302 (0.0541) lr 1.9098e-04 eta 0:08:34
epoch [10/10] batch [580/816] time 1.968 (2.009) data 0.001 (0.152) loss 0.0151 (0.0537) lr 1.9098e-04 eta 0:07:54
epoch [10/10] batch [600/816] time 1.941 (2.008) data 0.000 (0.147) loss 0.6499 (0.0547) lr 1.9098e-04 eta 0:07:13
epoch [10/10] batch [620/816] time 1.831 (2.001) data 0.001 (0.142) loss 0.0088 (0.0554) lr 1.9098e-04 eta 0:06:32
epoch [10/10] batch [640/816] time 1.820 (1.997) data 0.000 (0.138) loss 0.0050 (0.0565) lr 1.9098e-04 eta 0:05:51
epoch [10/10] batch [660/816] time 1.893 (1.994) data 0.001 (0.134) loss 0.0000 (0.0553) lr 1.9098e-04 eta 0:05:10
epoch [10/10] batch [680/816] time 1.814 (1.992) data 0.000 (0.130) loss 0.0030 (0.0556) lr 1.9098e-04 eta 0:04:30
epoch [10/10] batch [700/816] time 1.797 (1.989) data 0.000 (0.126) loss 0.0054 (0.0551) lr 1.9098e-04 eta 0:03:50
epoch [10/10] batch [720/816] time 1.912 (1.987) data 0.000 (0.122) loss 0.0464 (0.0540) lr 1.9098e-04 eta 0:03:10
epoch [10/10] batch [740/816] time 1.862 (1.986) data 0.000 (0.119) loss 0.0023 (0.0532) lr 1.9098e-04 eta 0:02:30
epoch [10/10] batch [760/816] time 2.275 (1.985) data 0.000 (0.116) loss 0.0001 (0.0531) lr 1.9098e-04 eta 0:01:51
epoch [10/10] batch [780/816] time 2.008 (1.983) data 0.000 (0.113) loss 0.0272 (0.0542) lr 1.9098e-04 eta 0:01:11
epoch [10/10] batch [800/816] time 1.783 (1.982) data 0.000 (0.110) loss 0.0927 (0.0548) lr 1.9098e-04 eta 0:00:31
Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/CoCoOp/vit_b16_c16_ep10_batch1_ctxv1/seed3\prompt_learner\model.pth.tar-10
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 1,053
* correct: 1,011
* accuracy: 96.0%
* error: 4.0%
* macro_f1: 96.0%
Elapsed: 4:54:33
